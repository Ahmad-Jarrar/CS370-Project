{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime as dt\n",
    "import random \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas_datareader.data import DataReader\n",
    "\n",
    "\n",
    "import gym\n",
    "from custom_environment import *\n",
    "from utils import *\n",
    "\n",
    "from stable_baselines3 import A2C, SAC, PPO, TD3, DDPG\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, steps=None, convert=False):\n",
    "\n",
    "    profits = []\n",
    "    sims = 10\n",
    "\n",
    "    sim_infos = []\n",
    "\n",
    "    for i in range(sims):\n",
    "        infos = []\n",
    "        obs = test_env.reset()\n",
    "        if steps == None:\n",
    "            while True:\n",
    "                if convert:\n",
    "                    action = model.predict(obs.to_numpy().reshape(-1))    \n",
    "                else:\n",
    "                    action, _states = model.predict(obs)\n",
    "                obs, rewards, done, info = test_env.step(action)\n",
    "                infos.append(info)\n",
    "                if done: \n",
    "                    profits.append(info['profit'])\n",
    "                    # test_env.render()\n",
    "                    sim_infos.append(infos)\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            for j in range(steps):\n",
    "                action, _states = model.predict(obs)\n",
    "                obs, rewards, done, info = test_env.step(action)\n",
    "                infos.append(info)\n",
    "                if done or j == steps-1: \n",
    "                    profits.append(info['profit'])\n",
    "                    # test_env.render()\n",
    "                    sim_infos.append(infos)\n",
    "                    break\n",
    "\n",
    "        print('finished sim %d/%d'%(i+1,sims))\n",
    "\n",
    "    pos_count = len(list(filter(lambda x: (x >= 0), profits))) \n",
    "    print('made profit - ' + str(pos_count/len(profits)))\n",
    "\n",
    "    return sim_infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading data\n",
      "              High          Low         Open        Close        Volume  \\\n",
      "count  5285.000000  5285.000000  5285.000000  5285.000000  5.285000e+03   \n",
      "mean     71.977223    69.307737    70.737365    70.635272  9.312302e+04   \n",
      "std      54.467648    52.628059    53.671556    53.556911  1.316300e+05   \n",
      "min       2.770000     2.520000     2.650000     2.550000  5.000000e+02   \n",
      "25%      13.700000    12.850000    13.350000    13.300000  2.362000e+04   \n",
      "50%      70.900002    68.349998    69.650002    69.550003  4.330000e+04   \n",
      "75%     114.949997   110.849998   113.099998   112.849998  1.042800e+05   \n",
      "max     237.500000   231.500000   235.000000   234.750000  2.127300e+06   \n",
      "\n",
      "         Adj Close  \n",
      "count  5285.000000  \n",
      "mean     31.200699  \n",
      "std      22.912915  \n",
      "min       2.328791  \n",
      "25%      10.283531  \n",
      "50%      24.550537  \n",
      "75%      51.769726  \n",
      "max      87.927231  \n"
     ]
    }
   ],
   "source": [
    "print('loading data')\n",
    "data = DataReader('GOGL', 'yahoo', start='2000-01-01', end='2021-01-01')\n",
    "\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.tail(365)\n",
    "train_data = data.head(-365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is a testing environment, scaling based on training data.\n",
      "C:\\Python38\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "C:\\Python38\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float16\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = CustomStockEnv(\n",
    "    stock_df = train_data,\n",
    "    pred_df = train_data,\n",
    "    window_size = 14,\n",
    "    initial_balance = 5000,\n",
    "    min_percent_loss = .5,\n",
    "    with_pred=False\n",
    "    )\n",
    "\n",
    "test_env = CustomStockEnv(\n",
    "    stock_df = test_data,\n",
    "    pred_df = test_data,\n",
    "    window_size = 14,\n",
    "    initial_balance = 5000,\n",
    "    min_percent_loss = .5,\n",
    "    with_pred=False,\n",
    "    test_env=True,\n",
    "    train_df=train_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training A2C\n",
      "done\n",
      "finished sim 1/10\n",
      "finished sim 2/10\n",
      "finished sim 3/10\n",
      "finished sim 4/10\n",
      "finished sim 5/10\n",
      "finished sim 6/10\n",
      "finished sim 7/10\n",
      "finished sim 8/10\n",
      "finished sim 9/10\n",
      "finished sim 10/10\n",
      "made profit - 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Training A2C\")\n",
    "\n",
    "env.reset()\n",
    "modelA2C = A2C('MlpPolicy', env, verbose=0)\n",
    "modelA2C.learn(total_timesteps=30000)\n",
    "\n",
    "print('done')\n",
    "modelA2C_info = evaluate(modelA2C)\n",
    "model_results[\"A2C\"] = modelA2C_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training PPO\n",
      "done\n",
      "finished sim 1/10\n",
      "finished sim 2/10\n",
      "finished sim 3/10\n",
      "finished sim 4/10\n",
      "finished sim 5/10\n",
      "finished sim 6/10\n",
      "finished sim 7/10\n",
      "finished sim 8/10\n",
      "finished sim 9/10\n",
      "finished sim 10/10\n",
      "made profit - 0.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Training PPO\")\n",
    "\n",
    "env.reset()\n",
    "modelPPO = PPO('MlpPolicy', env, verbose=0)\n",
    "modelPPO.learn(total_timesteps=30000)\n",
    "\n",
    "print('done')\n",
    "modelPPO_info = evaluate(modelPPO)\n",
    "model_results[\"PPO\"] = modelPPO_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training TD3\n",
      "C:\\Python38\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:199: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 0.69GB > 0.60GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "\"clamp_cpu\" not implemented for 'Half'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3651830565f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodelTD3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTD3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MlpPolicy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodelTD3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\stable_baselines3\\td3\\td3.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    193\u001b[0m     ) -> OffPolicyAlgorithm:\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         return super(TD3, self).learn(\n\u001b[0m\u001b[0;32m    196\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    281\u001b[0m                 \u001b[1;31m# do as many gradients steps as steps performed during the rollout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[0mgradient_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_steps\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_steps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrollout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_timesteps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\stable_baselines3\\td3\\td3.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[1;31m# Select action according to policy and add clipped noise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplay_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_policy_noise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                 \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_noise_clip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_noise_clip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m                 \u001b[0mnext_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_observations\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"clamp_cpu\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "print(\"Training TD3\")\n",
    "\n",
    "env.reset()\n",
    "modelTD3 = TD3('MlpPolicy', env, verbose=0)\n",
    "modelTD3.learn(total_timesteps=30000)\n",
    "\n",
    "print('done')\n",
    "modelTD3_info = evaluate(modelTD3)\n",
    "model_results[\"TD3\"] = modelTD3_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A2C\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-87239aaa6203>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmax_profit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'profit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# for result in results:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_profit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-87239aaa6203>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmax_profit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'profit'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# for result in results:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_profit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "for name, results in model_results.items():\n",
    "    print(name)\n",
    "    max_profit = max([info[['profit']] for info in results])\n",
    "    # for result in results:\n",
    "    print(max_profit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}